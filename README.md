## General Information

This repository contains the codebase for trained models performing [Major Entity Identification (MEI)](https://arxiv.org/abs/2406.14654). The code for few-shot LLM inference will be released soon.

### Repository Components:
1. **Coreference-based Models**: Baseline models using [longdoc](https://github.com/shtoshni/fast-coref) with inference-time mapping to entities.
2. **MEIRa Models**: Models with direct mappings to entities.

Hydra configurations are used across the project for setting up training for both Coreference and MEIRa models.

---

## Repository Structure

The repository includes the following folders:

- **`mei-data`**: Contains raw datasets for Coreference and MEI models, accessible from [Google Drive](https://drive.google.com/drive/folders/1vaVwHhMaDDXLw0rkLzTm5-AOSBDJVNJF?usp=sharing).
- **`longdoc`**: Code for training longdoc coreference models.
- **`MEIRa`**: Code for training MEIRa models.
- **`src`**: Code that utilizes trained models to generate MEI outputs.
- **`models`**: Stores coreference (coref_) and MEI (mei_) models.

---

## Environment Setup

To set up the environment, use the `environment.yaml` file provided. Alternatively, you can refer to `commands_env.txt` for a list of commands to manually set up the environment if needed.

---

## Models

Clone the models from Hugging Face repositories:

- **MEIRa-S**: [Hugging Face Link](https://huggingface.co/KawshikManikantan/meira-s)
- **MEIRa-H**: [Hugging Face Link](https://huggingface.co/KawshikManikantan/meira-h)

Create a `models` directory in the base directory and place the models inside.

---

## Data

Download the dataset from [Google Drive](https://drive.google.com/drive/folders/1vaVwHhMaDDXLw0rkLzTm5-AOSBDJVNJF?usp=sharing).

---

## Model Training

Training procedures follow those of [longdoc](https://github.com/shtoshni/fast-coref); refer to its README for details.

#### Training Configuration Keys:
- **`device`**: Specify the device for training (default: `cuda:0`).
- **`key`**: Unique identifier for labeling the experiment (default: null).

> **Note**: `{}` indicates a specific instance within a sub-configuration.

### Training Commands

#### Longdoc

**Template:**

```
cd longdoc/
python main.py experiment={experiment_name} model/doc_encoder/transformer={backbone_name} use_wandb={True/False} device={cpu/cuda:i/auto} key={label}
```

**Example:**
```
cd longdoc/
python main.py experiment=ontonotes_pseudo model/doc_encoder/transformer=longformer_large use_wandb=True device="cuda:1" key="onto_train"
```

### MEIRa

**Template:**

```
cd MEIRa/
python main.py experiment={experiment_name} model/doc_encoder/transformer={backbone_name} use_wandb={True/False} device={cpu/cuda:i/auto} key={label} model.memory.type={hybrid/static}
```

**Example:**
```
cd MEIRa/
python main.py experiment=ontonotes_pseudo model/doc_encoder/transformer=longformer_large use_wandb=True device="cuda:1" key="onto_train" model.memory.type=static
```

## Inference Guide (Dataset Scale)

### Step 1: Generate Coreference Clusters

Coreference clusters and other metadata generated by the model are stored in `model_path/{dataset_name}/{split}.log.jsonl`.

#### Using Coreference Models:

```bash
cd longdoc/
python main.py experiment={experiment_eval} model/doc_encoder/transformer={model_name} \
use_wandb=False train=False device={cpu/cuda:i/auto} paths.model_dir={model_dir}
```

#### Using MEIRa Models:

```bash
cd MEIRa/
python main.py experiment={experiment_eval} model/doc_encoder/transformer={model_name} \
use_wandb=False train=False device={cpu/cuda:i/auto} paths.model_dir={model_dir} \
model.memory.type={static/hybrid}
```

### Step 2: Process Clusters and Evaluate Performance

#### For Coreference Baselines

Coreference models provide clusters of all entities within the document. To map these clusters to specific entities, three mapping methods are available:

- **Coref-ID**: `coref_id/`
- **Coref-CosineMap**: `coref_cm/`
- **Coref-FuzzyMap**: `coref-fm/`

> **Note**: For **Coref-ID** mapping, run the following additional command before proceeding with processing:

```bash
cd MEIRa/
python main.py experiment={experiment_eval} model/doc_encoder/transformer={model_name} \
use_wandb=False train=False device={cpu/cuda:i/auto} paths.model_dir={model_dir} model.memory.type=hybrid log_dir_add="coref_id"
```

#### Evaluate Using Scripts

To evaluate with `src/evaluate/baselines.py` or `src/evaluate/meira.py`, review the configuration file `src/configs/args/args.yaml` for necessary options. You can also check examples in the `src/configs/args/experiments` folder for organizing evaluation configurations.

##### Coreference Evaluation Template

```bash
cd src/
python -m evaluate.baselines +experiments/{setup}={experiment_name} \
paths.model_base_path={model_path} mode={"coref_id/" / "coref_cm/" / "coref_fm"}
```

##### Coreference Evaluation Example

```bash
cd src/
python -m evaluate.baselines +experiments/e2e="lf_test" \
paths.model_base_path="../models/coref-onto" mode="coref_cm/"
```

#### For MEIRa Models

The MEIRa model output may include unnecessary metadata for analysis. To process and evaluate only the relevant cluster information, use the following template and example:

##### MEIRa Evaluation Template

```bash
cd src/
python -m evaluate.meira +experiments/{setup}={experiment_name} \
paths.model_base_path={model_path}
```

##### MEIRa Evaluation Example

```bash
cd src/
python -m evaluate.meira +experiments/e2e="lf_test" \
paths.model_base_path="../models/meira-s"
```
