{/mei-data/
 "cells": [/mei-data/
  {/mei-data/
   "cell_type": "cod/mei-data//mei-data/
   "executio/mei-data/t": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We create a split dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\") ## Go to src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets load the original aft dataset\n",
    "aft_documents = {}\n",
    "aft_me_dict = {}\n",
    "aft_jsonl_path = \"../data/raw_data/aft/longformer_speaker/test.4096.met.jsonlines\"\n",
    "aft_me = \"../data/raw_data/aft/longformer_speaker/test_major_entities.jsonl\"\n",
    "\n",
    "with jsonlines.open(aft_jsonl_path) as reader:\n",
    "    for obj in reader:\n",
    "        aft_documents[obj[\"doc_key\"]] = obj\n",
    "\n",
    "with jsonlines.open(aft_me) as reader:\n",
    "    for obj in reader:\n",
    "        aft_me_dict[obj[\"doc_key\"]] = obj\n",
    "        \n",
    "dynamic_keys = [\n",
    "    \"doc_key\",\n",
    "    \"clusters\",\n",
    "    \"num_clusters\",\n",
    "    \"representatives\",\n",
    "]\n",
    "\n",
    "static_keys = list(set(aft_documents[list(aft_documents.keys())[0]].keys()) - set(dynamic_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aft_increase_address = \"../data/raw_data/aft_increase/longformer_speaker/test.4096.met.jsonlines\"\n",
    "aft_increase_me_address = \"../data/raw_data/aft_increase/longformer_speaker/test_major_entities.jsonl\"\n",
    "aft_ind_address = \"../data/raw_data/aft_ind/longformer_speaker/test.4096.met.jsonlines\"\n",
    "aft_ind_me_address = \"../data/raw_data/aft_ind/longformer_speaker/test_major_entities.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_me_dataset(new_dataset_docs,aft_me_dict,mention_rep_map_list,new_me_address):\n",
    "    me_new_docs = []\n",
    "    for doc_id,doc in enumerate(new_dataset_docs):\n",
    "        base_document,mention_rep_map = mention_rep_map_list[doc_id]\n",
    "        me_doc = aft_me_dict[base_document]\n",
    "        me_new_doc = {\n",
    "            \"doc_key\": doc[\"doc_key\"],\n",
    "            \"cluster_inds\":[],\n",
    "            \"mention_inds\":[],\n",
    "            \"mention_strs\":[],\n",
    "        }\n",
    "        for cluster_ind,cluster in enumerate(doc[\"clusters\"][:-1]): ## Get for all clusters except the last one which is others\n",
    "            me_doc_index_cluster = None\n",
    "            for mention,me_doc_index in mention_rep_map.items():\n",
    "                if list(mention) in cluster:\n",
    "                    me_doc_index_cluster = me_doc_index\n",
    "                    break\n",
    "            if me_doc_index_cluster is not None:\n",
    "                me_new_doc[\"cluster_inds\"].append(me_doc[\"cluster_inds\"][me_doc_index_cluster])\n",
    "                me_new_doc[\"mention_inds\"].append(me_doc[\"mention_inds\"][me_doc_index_cluster])\n",
    "                me_new_doc[\"mention_strs\"].append(me_doc[\"mention_strs\"][me_doc_index_cluster])\n",
    "            else:\n",
    "                print(\"Representative Mention not found in cluster\")\n",
    "                print(\"Document ID: \",doc[\"doc_key\"])\n",
    "        me_new_docs.append(me_new_doc)\n",
    "    \n",
    "    with jsonlines.open(new_me_address, mode='w') as writer:\n",
    "        for doc in me_new_docs:\n",
    "            writer.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(244, 245): 0, (1230, 1231): 1, (377, 378): 2, (236, 238): 3, (4475, 4476): 4}\n",
      "{(175, 177): 0, (58, 59): 1, (5050, 5054): 2, (1944, 1947): 3, (579, 580): 4, (4132, 4133): 5, (141, 142): 6, (610, 611): 7, (728, 729): 8}\n",
      "{(0, 0): 0, (4709, 4710): 1, (162, 163): 2, (11333, 11335): 3, (12032, 12033): 4, (13365, 13368): 5, (1005, 1006): 6, (6154, 6156): 7, (9779, 9779): 8}\n"
     ]
    }
   ],
   "source": [
    "## Lets create a dataset with 1,2,3.....k entity documents\n",
    "aft_increase_documents = []\n",
    "mention_rep_map_list = []\n",
    "\n",
    "for document in aft_documents:\n",
    "    num_init_clusters = aft_documents[document][\"num_clusters\"]\n",
    "    num_major_entities = num_init_clusters -1\n",
    "\n",
    "    mentions_doc_gt = sorted([mention for cluster in aft_documents[document][\"clusters\"] for mention in cluster])\n",
    "    mention_rep_map = {tuple(mentions_doc_gt[mention_index_gt]): mention_rep_ind for mention_rep_ind,mention_index_gt in enumerate(aft_me_dict[document][\"mention_inds\"])}\n",
    "    print(mention_rep_map)\n",
    "        \n",
    "    sized_clusters, sized_reps = zip(*sorted(zip(aft_documents[document][\"clusters\"], aft_documents[document][\"representatives\"],), key=lambda x: len(x[0]), reverse=True))\n",
    "    ## Since the number of representatives is 1 less, zip removes the last element of the clusters\n",
    "    sized_clusters = list(sized_clusters)\n",
    "    sized_clusters.append(aft_documents[document][\"clusters\"][-1])\n",
    "    sized_reps = list(sized_reps)\n",
    "    \n",
    "    for num_entities in range(1,num_major_entities+1):\n",
    "        new_document = {}\n",
    "        new_document[\"doc_key\"] =  f\"{document}_{num_entities}\"\n",
    "        \n",
    "        for key in static_keys:\n",
    "            new_document[key] = aft_documents[document][key]\n",
    "         \n",
    "        new_document[\"num_clusters\"] = num_entities + 1\n",
    "        \n",
    "        new_clusters = sized_clusters[:num_entities]\n",
    "        others_cluster = []\n",
    "        for cluster in sized_clusters[num_entities:]:\n",
    "            others_cluster.extend(cluster)\n",
    "        new_clusters.append(sorted(others_cluster))\n",
    "        new_reps = sized_reps[:num_entities]\n",
    "        \n",
    "        ## Jointly sort the clusters and their representatives with respect to the representatives\n",
    "        new_sort_clusters, new_sort_reps = zip(*sorted(zip(new_clusters, new_reps), key=lambda x: x[1]))\n",
    "        new_sort_clusters = list(new_sort_clusters)\n",
    "        new_sort_clusters.append(new_clusters[-1])\n",
    "        new_sort_reps = list(new_sort_reps)\n",
    "        \n",
    "        new_document[\"clusters\"] = list(new_sort_clusters)\n",
    "        new_document[\"representatives\"] = list(new_sort_reps)\n",
    "        \n",
    "        aft_increase_documents.append(new_document)\n",
    "        mention_rep_map_list.append((document,mention_rep_map))\n",
    "\n",
    "## If directory of the file doesn't exist, create it\n",
    "if not os.path.exists(os.path.dirname(aft_increase_address)):\n",
    "    os.makedirs(os.path.dirname(aft_increase_address))\n",
    "    \n",
    "with jsonlines.open(aft_increase_address, mode='w') as writer:\n",
    "    for document in aft_increase_documents:\n",
    "        writer.write(document)\n",
    "\n",
    "generate_me_dataset(aft_increase_documents,aft_me_dict,mention_rep_map_list,aft_increase_me_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aladdin_0\n",
      "{(244, 245): 0, (1230, 1231): 1, (377, 378): 2, (236, 238): 3, (4475, 4476): 4}\n",
      "Alibaba_0\n",
      "{(175, 177): 0, (58, 59): 1, (5050, 5054): 2, (1944, 1947): 3, (579, 580): 4, (4132, 4133): 5, (141, 142): 6, (610, 611): 7, (728, 729): 8}\n",
      "alice_0\n",
      "{(0, 0): 0, (4709, 4710): 1, (162, 163): 2, (11333, 11335): 3, (12032, 12033): 4, (13365, 13368): 5, (1005, 1006): 6, (6154, 6156): 7, (9779, 9779): 8}\n"
     ]
    }
   ],
   "source": [
    "## Lets now create a dataset with individual entity documents\n",
    "aft_individual_documents = []\n",
    "mention_rep_map_list = []\n",
    "\n",
    "for document in aft_documents:\n",
    "    print(document)\n",
    "    num_init_clusters = aft_documents[document][\"num_clusters\"]\n",
    "    num_major_entities = num_init_clusters -1\n",
    "\n",
    "    mentions_doc_gt = sorted([mention for cluster in aft_documents[document][\"clusters\"] for mention in cluster])\n",
    "    mention_rep_map = {tuple(mentions_doc_gt[mention_index_gt]): mention_rep_ind for mention_rep_ind,mention_index_gt in enumerate(aft_me_dict[document][\"mention_inds\"])}\n",
    "    print(mention_rep_map)\n",
    "    \n",
    "    sized_clusters, sized_reps = zip(*sorted(zip(aft_documents[document][\"clusters\"], aft_documents[document][\"representatives\"],), key=lambda x: len(x[0]), reverse=True))\n",
    "    ## Since the number of representatives is 1 less, zip removes the last element of the clusters\n",
    "    sized_clusters = list(sized_clusters)\n",
    "    sized_clusters.append(aft_documents[document][\"clusters\"][-1])\n",
    "    sized_reps = list(sized_reps)\n",
    "    \n",
    "    for entity_id in range(0,num_major_entities):\n",
    "        new_document = {}\n",
    "        new_document[\"doc_key\"] =  f\"{document}_{entity_id}\"\n",
    "        \n",
    "        for key in static_keys:\n",
    "            new_document[key] = aft_documents[document][key]\n",
    "         \n",
    "        new_document[\"num_clusters\"] = 2\n",
    "        \n",
    "        new_clusters = [sized_clusters[entity_id]]\n",
    "        others_cluster = []\n",
    "        for cluster_ind,cluster in enumerate(sized_clusters):\n",
    "            if cluster_ind != entity_id:\n",
    "                others_cluster.extend(cluster)\n",
    "        new_clusters.append(sorted(others_cluster))\n",
    "        new_reps = [sized_reps[entity_id]]\n",
    "        \n",
    "        new_document[\"clusters\"] = list(new_clusters)\n",
    "        new_document[\"representatives\"] = list(new_reps)\n",
    "        \n",
    "        aft_individual_documents.append(new_document)\n",
    "        mention_rep_map_list.append((document,mention_rep_map))\n",
    "\n",
    "## If directory of the file doesn't exist, create it\n",
    "if not os.path.exists(os.path.dirname(aft_ind_address)):\n",
    "    os.makedirs(os.path.dirname(aft_ind_address))\n",
    "    \n",
    "with jsonlines.open(aft_ind_address, mode='w') as writer:\n",
    "    for document in aft_individual_documents:\n",
    "        writer.write(document)\n",
    "\n",
    "generate_me_dataset(aft_individual_documents,aft_me_dict,mention_rep_map_list,aft_ind_me_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
